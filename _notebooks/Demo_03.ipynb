{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-tJw6qgMOho"
      },
      "source": [
        "# Notebook 3 - Classification and Working With Text\n",
        "\n",
        "This notebook is heavily indebetted to [Prof. Aaron Culotta's NLP Course](https://www.cs.tulane.edu//~aculotta/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## This is for CoLab: clone the course data and change to the correct\n",
        "## directory on CoLab\n",
        "# clone the course repository, change to right directory, and import libraries.\n",
        "%cd /content\n",
        "!git clone https://github.com/TulaneCS/intd2810.git\n",
        "%cd /content/intd2810/_notebooks"
      ],
      "metadata": {
        "id": "o5uRHQPmZEMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Natural vs. Unnatural (Formal) Languages\n",
        "\n",
        "**Natural**\n",
        "- Emerges from intelligent beings\n",
        "- We **discover** the grammar.\n",
        "- Full of ambiguity\n",
        "- English, Spanish, Dolphin Language?\n",
        "\n",
        "**Formal**\n",
        "- Defined by humans\n",
        "- We **prescribe** the grammar.\n",
        "- Designed to **remove** ambiguity\n",
        "- Python, math, ...\n",
        "<br><br><br><br><br><br>"
      ],
      "metadata": {
        "id": "TTJy9W5V9vQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are some examples of NLP?\n",
        "\n",
        "![figs/watson.jpg](https://github.com/tulane-cmps6730/main/blob/main/lec/overview/figs/watson.jpg?raw=1)\n",
        "<br><br><br><br><br><br>\n",
        "\n",
        "![figs/siri.png](https://github.com/tulane-cmps6730/main/blob/main/lec/overview/figs/siri.png?raw=1)\n",
        "<br><br><br><br><br><br>\n",
        "\n",
        "![figs/translate.jpg](https://github.com/tulane-cmps6730/main/blob/main/lec/overview/figs/translate.jpg?raw=1)\n",
        "<br><br><br><br><br><br>\n",
        "\n",
        "![figs/echo.jpg](https://github.com/tulane-cmps6730/main/blob/main/lec/overview/figs/echo.jpg?raw=1)\n",
        "<br><br><br><br><br><br>\n",
        "\n",
        "![figs/her.jpg](https://github.com/tulane-cmps6730/main/blob/main/lec/overview/figs/her.jpg?raw=1)\n",
        "\n",
        "![figs/chatgpt.png](https://github.com/tulane-cmps6730/main/blob/main/lec/overview/figs/chatgpt.png?raw=1)\n",
        "<br><br><br><br><br><br>"
      ],
      "metadata": {
        "id": "V4so4nt892Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is NLP??\n",
        "\n",
        "<br><br><br><br>\n",
        "\n",
        "**Natural Language Processing = Linguistics + Logic + Machine Learning**\n",
        "\n",
        "Each has a long history...\n",
        "\n",
        "<br><br><br><br>\n",
        "\n",
        "## NLP History\n",
        "\n",
        "- Dates back to first days of computing (Turing)\n",
        "- Combines linguistics, formal logic, and statistics\n",
        "\n",
        "### 1940s-1950s\n",
        "\n",
        "**Logic**\n",
        "- automaton (Turing)\n",
        "- Context-free grammars (Chomsky)\n",
        "\n",
        "**Statistics**\n",
        "- McCulloch Pits neuron (perceptron)\n",
        "- Noisy channel model (Shannon)\n",
        "\n",
        "### Turing test (Alan Turing, 1950)\n",
        "\n",
        "\n",
        "Q: Please write me a sonnet on the topic of the Forth Bridge.  \n",
        "A: Count me out on this one. I never could write poetry  \n",
        "\n",
        "\n",
        "Q: Add 34957 to 70764  \n",
        "A: (Pause about 30 seconds and then give answer as) 105721.\n",
        "\n",
        "<br><br><br><br>\n",
        "\n",
        "### 1957-1970\n",
        "\n",
        "**Logic**\n",
        "- Theorem proving (Newell & Simon)\n",
        "- Logical inference\n",
        "\n",
        "**Statistics**\n",
        "- Neural nets\n",
        "- Bayesian statistics (e.g., identify author of text)\n",
        "\n",
        "\n",
        "#### ELIZA (1964, Weizenbaum)\n",
        "\n",
        "User: You are like my father in some ways.  \n",
        "ELIZA: WHAT RESEMBLANCE DO YOU SEE  \n",
        "User: You are not very aggressive but I think you don’t want me to notice that.  \n",
        "ELIZA: WHAT MAKES YOU THINK I AM NOT AGGRESSIVE  \n",
        "User: You don’t argue with me.  \n",
        "ELIZA: WHY DO YOU THINK I DON’T ARGUE WITH YOU?  \n",
        "User: You are afraid of me.  \n",
        "ELIZA: DOES IT PLEASE YOU TO BELIEVE I AM AFRAID OF YOU\n",
        "\n",
        "<br><br>\n",
        "\n",
        "Online example: <http://www.masswerk.at/elizabot/>\n",
        "<br><br><br><br>\n",
        "\n",
        "### 1970-1983\n",
        "\n",
        "**Logic**\n",
        "  - Winograd's SHRDLU\n",
        "\n",
        "**Statistics**\n",
        "  - speech recognition (AT&T Bell Labs, IBM)\n",
        "  - Hidden Markov Models\n",
        "\n",
        "![figs/shrdlu.png](https://github.com/tulane-cmps6730/main/blob/main/lec/overview/figs/shrdlu.png?raw=1)\n",
        "\n",
        "\n",
        "### 1983-1993\n",
        "- Return to finite state models\n",
        "- Empirical approach: (data driven) IBM speech recognition\n",
        "\n",
        "### 1994-1999\n",
        "- Empirical approach widespread\n",
        "- Bayesian statistics\n",
        "- graphical models\n",
        "\n",
        "### 2000s\n",
        "- Combinations of logical and empirical approaches\n",
        "  - Markov logic networks, etc.\n",
        "- Deep learning\n",
        "  - revival of neural nets from 1960s\n",
        "<br><br><br><br><br><br>\n",
        "\n",
        "![figs/gpt_growth.png](https://github.com/tulane-cmps6730/main/blob/main/lec/overview/figs/gpt_growth.png?raw=1)\n",
        "\n",
        "(Parmida Beigi, Amazon)"
      ],
      "metadata": {
        "id": "4Lvm35rsX3Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why is NLP Hard?\n",
        "\n",
        "### Ambiguity: The Good and the Bad\n",
        "\n",
        "- Makes language fun and interesting for humans, but makes language difficult for computers.\n",
        "- The central problem to NLP is **resolving ambiguity**.\n",
        "\n",
        "\n",
        "- E.g., \"*I made her duck*.\"\n",
        "\n",
        "<br><br><br><br><br><br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "1. I cooked waterfowl for her.\n",
        "2. I cooked waterfowl belonging to her.\n",
        "3. I created the (plaster?) duck she owns.\n",
        "4. I caused her to quickly lower her head or body.\n",
        "5. I waved my magic wand and turned her into undifferentiated waterfowl.\n",
        "\n",
        "\n",
        "- Syntactic ambiguity (1 vs 4): \"duck\" $\\rightarrow$ verb or noun?  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **part-of-speech tagging, syntactic parsing**\n",
        "- Semantic ambiguity (1 vs 3): \"make\" $\\rightarrow$ *create* or *cook*? &nbsp;&nbsp; **word sense disambiguation**\n",
        "- Phonetic ambiguity: \"I\" or \"eye\"; \"made\" or \"maid\"?  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **speech recognition**"
      ],
      "metadata": {
        "id": "62vC_ZgeYWZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Little Bit of Machine Learning\n",
        "\n",
        "<img src='https://github.com/tulane-cmps6730/main/blob/main/lec/classify/figs/spam.png?raw=1'/>"
      ],
      "metadata": {
        "id": "sFmDnr_SWRWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A little bit of notation...\n",
        "\n",
        "- $\\vec{x} \\in X$ &nbsp;&nbsp;&nbsp;&nbsp; *instance*, *example*, *input*\n",
        "  - e.g., an email\n",
        "  \n",
        "  \n",
        "- $y \\in Y$ &nbsp;&nbsp;&nbsp;&nbsp; *target*, *class*, *label*, *output*\n",
        "  - e.g., $y=1$: spam ; $y=0$: not spam\n",
        "  \n",
        "  \n",
        "- $f: X \\mapsto Y$ &nbsp;&nbsp;&nbsp;&nbsp; *hypothesis*, *learner*, *model*, *classifier*\n",
        "  - e.g., if $x$ contain the word *free*, $y$ is $1$.\n",
        "\n",
        "### Important Problems\n",
        "\n",
        "- **Classification**\n",
        "  - $\\vec{x}$: email ;  $y$: spam or not\n",
        "- **Regression**\n",
        "  - $\\vec{x}$: twitter feed of a person ; $y$: age\n",
        "- **Clustering**\n",
        "  - $\\vec{x}$: news articles ; $y$: topics\n",
        "\n",
        "### The Basic Workflow\n",
        "\n",
        "1. **Collect** raw data: emails\n",
        "2. Manually **categorize** them:  spam or not\n",
        "3. **Vectorize**: email -> word counts [**features**]\n",
        "4. **Train** / **Fit**: create $f(x)$\n",
        "5. **Collect** new raw data\n",
        "6. **Predict**: compute $f(x)$ for new $x$\n"
      ],
      "metadata": {
        "id": "Dbqr9RyJWj80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Spam Classification\n",
        "\n",
        "**Steps 1 & 2: Collect and categorize**\n",
        "\n",
        "**Spam:**\n",
        "\n",
        "> Free credit report!\n",
        "\n",
        "\n",
        "> Free money!\n",
        "\n",
        "\n",
        "**Not spam:**\n",
        "\n",
        "> Are you free tonight?\n",
        "\n",
        "> How are you?\n",
        "\n",
        "\n",
        "**Step 3: Vectorize**\n",
        "\n",
        "> 'Free money!'\n",
        "\n",
        "becomes\n",
        "\n",
        "```\n",
        "free: 1\n",
        "money: 1\n",
        "!: 1\n",
        "?: 0\n",
        "credit: 0\n",
        "...\n",
        "```\n",
        "\n",
        "**Representation**: \"Feature engineering is the key\" -- Domingos\n",
        "\n",
        "Why is this (seemingly) a terrible representation of a document?\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "When working with text we're going to use basic logistic regression and a Bag of words model.\n",
        "\n",
        "**Bag of Words**\n",
        "\n",
        "![bow](https://github.com/tulane-cmps6730/main/blob/main/lec/classify/figs/bow.png?raw=1)\n"
      ],
      "metadata": {
        "id": "UeGVAKSOXEtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Train/Fit**\n",
        "\n",
        "Which model to use?\n",
        "\n",
        "- Naive Bayes\n",
        "- Logistic Regression\n",
        "- Decision Tree\n",
        "- K-Nearest Neighbors\n",
        "- Support Vector Machines\n",
        "- (Deep) Neural Networks\n",
        "- ... many many more\n",
        "\n",
        "**Steps 5-6: Predict on new data**\n",
        "\n",
        "> Free vacation!\n",
        "\n",
        "**Spam**\n",
        "\n",
        "But... How do you know if it works???"
      ],
      "metadata": {
        "id": "CcO8kym3Yqzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Make the fonts a little bigger in our graphs.\n",
        "font = {'size'   : 14}\n",
        "plt.rc('font', **font)\n",
        "plt.rcParams['mathtext.fontset'] = 'cm'\n",
        "plt.rcParams['pdf.fonttype'] = 42"
      ],
      "metadata": {
        "id": "PRJoysGEZTOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Simple Classifier... and why it's hard..\n",
        "\n",
        "**Spam:**\n",
        "\n",
        "> Free free credit report free!\n",
        "\n",
        "> Cousin, are you free for free today?\n",
        "\n",
        "> Free money!\n",
        "\n",
        "\n",
        "**Not spam:**\n",
        "\n",
        "> Are you available tonight?\n",
        "\n",
        "> Cousin how are you?"
      ],
      "metadata": {
        "id": "LAaMzVl1Y3RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X: each row is a feature vector for one document.\n",
        "data = pd.DataFrame([\n",
        "    (0,0,0),\n",
        "    (1,0,0),\n",
        "    (0,3,1),\n",
        "    (1,3,1),\n",
        "  ],columns=['cousin', 'free', 'y'])\n",
        "data"
      ],
      "metadata": {
        "id": "Kp7BZKlzZImo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q hvplot\n",
        "import hvplot.pandas\n",
        "import holoviews as hv\n",
        "from matplotlib.colors import ListedColormap\n",
        "cmap = ListedColormap(['blue', 'red'])\n",
        "hv.extension('bokeh')\n",
        "data.hvplot(x='cousin', y='free', kind='scatter', c='y', cmap=cmap, s=200)"
      ],
      "metadata": {
        "id": "dMC-xJ8rZwmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We often separate the data into feature matrix X and the target vector y\n",
        "X = data[['cousin', 'free']].values\n",
        "y = data['y'].values\n",
        "display(X)\n",
        "display(y)"
      ],
      "metadata": {
        "id": "A0v96mRYavn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplest machine learning algorithm:\n",
        "\n",
        "class SimplestMachine:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.f = dict()\n",
        "\n",
        "    def train(self, X, y):\n",
        "        for xi, yi in zip(X, y):\n",
        "            self.f[tuple(xi)] = yi\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.f[tuple(x)]\n",
        "\n",
        "# What does this do?"
      ],
      "metadata": {
        "id": "kZ8lkhz7a8C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does zip do?\n",
        "[x for x in zip([1, 2, 3], ['a', 'b', 'c', 'd'])]"
      ],
      "metadata": {
        "id": "RwmH98QRa-HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the classifier\n",
        "simplest_machine = SimplestMachine()\n",
        "# train the classifier\n",
        "simplest_machine.train(X, y)\n",
        "# predict\n",
        "data['prediction'] = [simplest_machine.predict(xi) for xi in X]\n",
        "data"
      ],
      "metadata": {
        "id": "YSbVoQCPa-Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does it do for unseen example?\n",
        "simplest_machine.predict((0, 4))"
      ],
      "metadata": {
        "id": "34fS_sTda966"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second simplest machine learning algorithm:\n",
        "class SimpleMachine:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.f = dict()\n",
        "\n",
        "    def train(self, X, y):\n",
        "        for xi, yi in zip(X, y):\n",
        "            self.f[tuple(xi)] = yi\n",
        "\n",
        "    def predict(self, x):\n",
        "        x_closest = self.find_most_similar(x)\n",
        "        return self.f[x_closest]\n",
        "\n",
        "    def find_most_similar(self, x):\n",
        "        distances = [self.distance(x, xi) for xi in self.f.keys()]\n",
        "        best_idx = np.argmin(distances)\n",
        "        return list(self.f.keys())[best_idx]\n",
        "\n",
        "    def distance(self, x, xi):\n",
        "        return np.sqrt(np.sum((np.array(x)-np.array(xi))**2))\n",
        "\n",
        "# What does this do?"
      ],
      "metadata": {
        "id": "s4lfeQ7SbKPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Euclidean distance:**   \n",
        "\n",
        "```\n",
        "(0, 3)\n",
        "(1, 5)\n",
        "```\n",
        "\n",
        "$$\\sqrt{(0-1)^2 + (3-5)^2} = \\sqrt{5}$$"
      ],
      "metadata": {
        "id": "5Sb9vBqLbNsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_machine = SimpleMachine()\n",
        "simple_machine.train(X, y)\n",
        "data['prediction'] = [simple_machine.predict(xi) for xi in X]\n",
        "data"
      ],
      "metadata": {
        "id": "SB2BsVXAbRDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does it do for unseen example?\n",
        "print(simple_machine.predict((0, 4)))"
      ],
      "metadata": {
        "id": "top_YRsVbV6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/tulane-cmps6730/main/blob/main/lec/classify/figs/knn.png?raw=1' width='80%'/>\n",
        "\n",
        "<http://www.scholarpedia.org/article/K-nearest_neighbor>"
      ],
      "metadata": {
        "id": "eLxQN6v_baq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generalization\n",
        "\n",
        "How accurate will I be on a new, unobserved example?\n",
        "\n",
        "How do you know if it works?\n",
        "\n",
        "1. Train on data ${D_1}$\n",
        "2. Predict on data ${D_2}$\n",
        "3. Compute accuracy on ${D_2}$.\n",
        "   - Why not ${D_1}$?\n",
        "\n",
        "How do you know if it works?\n",
        "\n",
        "1. Train on data ${D_1}$\n",
        "2. Predict on data ${D_2}$\n",
        "3. Compute accuracy on ${D_2}$.\n",
        "4. Tweak algorithm / representation\n",
        "5. Repeat\n",
        "\n",
        "How do you know if it works?\n",
        "\n",
        "1. Train on data ${D_1}$\n",
        "2. Predict on data ${D_2}$\n",
        "3. Compute accuracy on ${D_2}$.\n",
        "4. Tweak algorithm / representation\n",
        "5. Repeat\n",
        "\n",
        "How many times can I do this?\n",
        "\n",
        "#### Measuring Generalization\n",
        "\n",
        "- Cross-validation\n",
        "  - train on 90%, test on 10%, repeat 10 x's\n",
        "       - each example appears only once in test set\n",
        "       \n",
        "       \n",
        "<p><a href=\"https://commons.wikimedia.org/wiki/File:K-fold_cross_validation_EN.svg#/media/File:K-fold_cross_validation_EN.svg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/K-fold_cross_validation_EN.svg/1200px-K-fold_cross_validation_EN.svg.png\" alt=\"K-fold cross validation EN.svg\"></a><br><font size=-2>By <a href=\"//commons.wikimedia.org/w/index.php?title=User:Gufosowa&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"User:Gufosowa (page does not exist)\">Gufosowa</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=82298768\">Link</a></font></p>"
      ],
      "metadata": {
        "id": "uWNikhb-bQz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Experimental Design\n",
        "\n",
        "1. Collect data\n",
        "2. Build model\n",
        "3. Compute cross-validation accuracy\n",
        "4. Tune model\n",
        "5. Repeat\n",
        "6. **Report accuracy on new data**\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "- What is overfitting? How do you know it is happening? How do you fix?\n",
        "\n",
        "\n",
        "<img src=\"https://hackernoon.com/hn-images/1*SBUK2QEfCP-zvJmKm14wGQ.png\"/>\n",
        "\n",
        "\n",
        "<img src=\"https://hackernoon.com/hn-images/1*xWfbNW3arf39wxk4ZkI2Mw.png\"/>\n",
        "\n",
        "[source](https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42)\n",
        "\n",
        "\n",
        "If overfitting:\n",
        "- get more labeled data\n",
        "- reduce complexity of model (fewer parameters)\n",
        "- stop the training function early\n",
        "\n",
        "If underfitting:\n",
        "- increase complexity of model (more parameters)\n",
        "- let the training function run longer\n"
      ],
      "metadata": {
        "id": "J-OguMQ3b9iJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN1g8kQd_QKv"
      },
      "source": [
        "## Text Classification\n",
        "\n",
        "In this example we go through a light example of processing a dataset for analyzing text.\n",
        "\n",
        "The data comes from [this website](https://www.cs.cornell.edu/people/pabo/movie-review-data/) at Cornell and is from Bo Pang and Lillian Lee, A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts, Proceedings of ACL 2004.\n",
        "\n",
        "This contains 1000 positive and 1000 negative movie reviews. Our job is to classify a review as positive or negative based on the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJYg4wgXAYxK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# need to unzip the data first.\n",
        "!unzip ./data/review_polarity.zip -d ./data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2-W8PCCArxJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!ls data/review_polarity/pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkjx-0p3_Wvz"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "# labels are based on which directory the files are in.\n",
        "all_pos = list(glob.glob(\"./data/review_polarity/pos/*\"))\n",
        "all_neg = list(glob.glob(\"./data/review_polarity/neg/*\"))\n",
        "labels = np.array([1] * len(all_pos) + [0] * len(all_neg))\n",
        "filenames = all_pos + all_neg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at a \"positive\" and \"negative\"\n",
        "print(filenames[0])\n",
        "print(filenames[1001])"
      ],
      "metadata": {
        "id": "zobvFBp-PsJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ./data/review_polarity/pos/cv172_11131.txt"
      ],
      "metadata": {
        "id": "HyLUEtfNUUaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ./data/review_polarity/neg/cv833_11961.txt"
      ],
      "metadata": {
        "id": "_2jYn0dTUa3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atBZM4FWI2x4"
      },
      "source": [
        "We'll use TfidfVectorizer to convert each document into a (sparse) *feature* vector.\n",
        "\n",
        "As part of this we pass in:\n",
        "\n",
        "**ngram_rangetuple (min_n, max_n), default=(1, 1)**\n",
        "\n",
        "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU1axtAEAYM6"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "vec = TfidfVectorizer(input='filename', stop_words='english', ngram_range=(1, 1))\n",
        "X = vec.fit_transform(filenames)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpsQVY9DDf4h"
      },
      "source": [
        "So, we have 2000 documents and 39,354 unique words.\n",
        "\n",
        "How big is this matrix?\n",
        "\n",
        "Wait, how do we store that?\n",
        "\n",
        "dense matrix:\n",
        "$$\n",
        "X=\n",
        "  \\begin{bmatrix}\n",
        "    0.1 & 2.8 & 3.2 & ... & 1.5 \\\\\n",
        "    3.2 & 4.1 & 5.1 & ... & 2.7  \\\\\n",
        "    ...\\\\\n",
        "    1.4 & 3.4 & 7.5 & ... & 7.5  \\\\\n",
        "  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "sparse matrix:\n",
        "$$\n",
        "X=\n",
        "  \\begin{bmatrix}\n",
        "    0.1 & 0 & 0 & ... & 1.5 \\\\\n",
        "    0 & 0 & 0 & ... & 2.7  \\\\\n",
        "    ...\\\\\n",
        "    0 & 3.4 & 0 & ... & 0  \\\\\n",
        "  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "How can we store a sparse matrix more efficiently?\n",
        "\n",
        "<br><br><br>\n",
        "[CSR matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT0AxAWxDnn6"
      },
      "outputs": [],
      "source": [
        "X[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vfou1bkiEADv"
      },
      "outputs": [],
      "source": [
        "filenames[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TclbqfRcDuAo"
      },
      "outputs": [],
      "source": [
        "!cat ./data/review_polarity/pos/cv172_11131.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPPKb_19EYEb"
      },
      "outputs": [],
      "source": [
        "X[0].indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1PM1PS9Ea2O"
      },
      "outputs": [],
      "source": [
        "feature_names = np.array(vec.get_feature_names_out())\n",
        "feature_names[X[0].indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mVc4jNUErM6"
      },
      "outputs": [],
      "source": [
        "X[0].data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyCHOyOME_bK"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.4,\n",
        "                                                    shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRqkGnsMEz-v"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as metrics\n",
        "textlr = LogisticRegression()\n",
        "textlr.fit(X_train, y_train)\n",
        "y_predicted = textlr.predict(X_test)\n",
        "print(f\"accuracy= {metrics.accuracy_score(y_predicted,y_test):.3f}\")\n",
        "print(f\"precision= {metrics.precision_score(y_predicted,y_test):.3f}\")\n",
        "print(f\"recall ={metrics.recall_score(y_predicted,y_test):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdDPsO-YFOj9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "ConfusionMatrixDisplay.from_estimator(textlr, X_test, y_test,\n",
        "                                        display_labels=[\"Negative\", \"Positive\"],\n",
        "                                        cmap=plt.cm.Blues, normalize='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IMBC_pLGHn4"
      },
      "outputs": [],
      "source": [
        "coefficient_list = pd.DataFrame(textlr.coef_[0],  index=feature_names).rename(columns={0: 'coef'})\n",
        "coefficient_list.sort_values('coef', ascending=False).head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EISw8DspHICB"
      },
      "outputs": [],
      "source": [
        "coefficient_list.sort_values('coef', ascending=True).head(20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}