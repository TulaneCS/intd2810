{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZp7HiD3Uf23"
      },
      "source": [
        "# Notebook 4 - Language Models\n",
        "\n",
        "In this notebook we'll go through the process of building a small n-gram language model that makes some very famous Tweets\n",
        "\n",
        "This builds very heavily on the work of [Aron Culotta](https://cs.tulane.edu/~aculotta/) at Tulane University."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqRrAPIvUzDl"
      },
      "source": [
        "- How do we score one sentence as more likely than another?\n",
        "- Will use some probability:\n",
        "  - $P($\"I'm sorry Dave\"$) > P($ \"Sorry Dave I'm\" $)$\n",
        "- How can we model the probability of a sentence?\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "- Why is it important to be able to score the probability of sentences?\n",
        "\n",
        "<br><br><br><br>\n",
        "\n",
        "Example: **Predictive text**\n",
        "\n",
        "![figs/predict.png](https://github.com/tulane-cmps6730/main/blob/main/lec/language_models/figs/predict.png?raw=1)\n",
        "\n",
        "![figs/unicorn.jpg](https://github.com/tulane-cmps6730/main/blob/main/lec/language_models/figs/unicorn.jpg?raw=1)\n",
        "\n",
        "![figs/now.jpg](https://github.com/tulane-cmps6730/main/blob/main/lec/language_models/figs/now.jpg?raw=1)\n",
        "\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "**Many other applications:**\n",
        "\n",
        "\n",
        "- Machine translation\n",
        "\n",
        "![tranl](https://github.com/tulane-cmps6730/main/blob/main/lec/language_models/figs/tranl.png?raw=1)\n",
        "\n",
        "> $P($ \"he briefed reporters on the main contents of the statement\" $) $ $ > $  \n",
        "> $P($ \"he briefed to reporters the main contents of the statement\" $)$\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "- Speech recognition\n",
        "\n",
        "> $P($ \"I am hungry\" $) $ $ > P($ \"Eye am hungry\" $)$\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "- Language generation\n",
        "\n",
        "![kingcake](https://github.com/tulane-cmps6730/main/blob/main/lec/language_models/figs/kingcake.png?raw=1)\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "**More generally:**\n",
        "\n",
        "- Due to the prevalence of ambiguity, we need a way to rank probable interpretations of a sentence\n",
        "- In the future, we'll look at probability of parses\n",
        "- Today, we'll start with a simpler task\n",
        "  - The probability of a sentence (word sequence).\n",
        "\n",
        "<br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YWOQOxwVCl4"
      },
      "source": [
        "$p(w_1 \\ldots w_m)=$?\n",
        "\n",
        "E.g., $p($\"Sam I am.$\")=?$\n",
        "\n",
        "\n",
        "Given a large sample of sentences $D$, how can we estimate the probability of one particular sentence?\n",
        "\n",
        "<br><br>\n",
        "**Analogy**: Given a sample of $n$ people, how do we estimate $p($ brown eyes $)$?\n",
        "\n",
        "<br><br>\n",
        "$p($ brown eyes $)= \\#($brown eyes$) $ $ / $ $n$  \n",
        "= \"the fraction of all people that have brown eyes\"\n",
        "\n",
        "\n",
        "<br>\n",
        "So, returning to the above,\n",
        "\n",
        "$p($Sam I am.$)=$ the fraction of all sentences that equal \"Sam I am.\"\n",
        "\n",
        "**Why is this a bad idea?**\n",
        "\n",
        "<br><br><br><br>\n",
        "Most sentences occur only once.  \n",
        "$p(w_1 \\ldots w_m) \\approx \\frac{1}{n}$\n",
        "\n",
        "Any sentence not in $D$ will have probability 0.\n",
        "\n",
        "Instead, to make estimating these probabilities possible, we need to make assumptions. E.g.,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G3ftSByVG0c"
      },
      "source": [
        "## Independnece Assumption\n",
        "\n",
        "Recall from probability: if $A$ and $B$ are independent events, then $P(A,B) = P(A) * P(B)$\n",
        "- E.g., P(\"It will rain tomorrow in Chicago\" AND \" My knee hurts today\") =  \n",
        "P(\"It will rain tomorrow in Chicago\") * P(\"My knee hurts today\")\n",
        "\n",
        "If we assume that each word in a sentence is an independent event then:\n",
        "\n",
        "$p(w_1 \\ldots w_m) \\approx p(w_1) * p(w_2) * \\ldots p(w_m)$\n",
        "\n",
        "(Decompose the joint probability of the words in a sentence into the product of individual word probabilities.)\n",
        "\n",
        "\n",
        "\n",
        "This is a bad assumption: **why?**\n",
        "<br><br><br><br>\n",
        "- \"San Francisco\", \"love you\", ...\n",
        "\n",
        "- we will return to this problem later today\n",
        "\n",
        "But first: how do we estimate the probability of a single word $p(w_i)$ given a dataset $D$?\n",
        "<br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpqXs_MWVVkA"
      },
      "source": [
        "#Estimating Unigram Probabilities\n",
        "\n",
        "\n",
        "**unigram:** a single word; or, a phrase of length 1.\n",
        "\n",
        "To estimate unigram probability from some dataset of text, just count the word frequency:\n",
        "\n",
        "$$ p(w_i) = \\frac{C(w_i)}{T} $$\n",
        "\n",
        "- $C(w_i)=$ number of times word $w_i$ appears in data\n",
        "- $T=$ number of tokens in data\n",
        "\n",
        "Recall definition of probability:\n",
        "- $0 \\le p(w_i) \\le 1$ $ \\forall i$\n",
        "- $\\sum_i p(w_i) = 1$\n",
        "\n",
        "This is an example of a **<font color=\"blue\">multinomial distribution</font>**:\n",
        "- A distribution over $n$ discrete events\n",
        "- In each trial, only one event can occur.\n",
        "- E.g., roll of an $n$-sided die.\n",
        "- Imagine a giant die with a word on each face. Each word is weighted by its probability. We can sample words by rolling the die.\n",
        "\n",
        "<br>\n",
        "\n",
        "```\n",
        "<s> I am Sam </s>  \n",
        "<s> Sam I am </s>  \n",
        "<s> I do not like green eggs and ham </s>\n",
        "```\n",
        "\n",
        "- $p($I$)$ $ = \\frac{3}{20} = 0.15$\n",
        "- $p($Sam$)$ $ = \\frac{2}{20} = 0.1$\n",
        "- ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb7b_DSwVhey"
      },
      "source": [
        "## Unigram Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaJ_nGXNUoq_"
      },
      "outputs": [],
      "source": [
        "# common imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import array as npa\n",
        "import pandas as pd\n",
        "# Counter: an enhanced dict for counting\n",
        "# https://docs.python.org/3/library/collections.html\n",
        "from collections import Counter\n",
        "\n",
        "d = Counter()\n",
        "d.update(['a', 'b', 'c'])\n",
        "d.update(['a', 'a', 'b'])\n",
        "d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2KawMw0Vkv9"
      },
      "outputs": [],
      "source": [
        "# Example of estimating unigram probabilities from a set of documents.\n",
        "\n",
        "docs = ['<s> I am Sam </s>',\n",
        "        '<s> Sam I am </s>',\n",
        "        '<s> I do not like green eggs and ham </s>']\n",
        "\n",
        "def estimate_unigram_probs(docs):\n",
        "    \"\"\"\n",
        "    Compute p(w), the probability of each unigram in this data.\n",
        "    p(w) = count(w) / total_num_tokens\n",
        "    \"\"\"\n",
        "    counts = Counter()\n",
        "    for doc in docs:\n",
        "        tokens = doc.split()   # split sentence into words using spaces\n",
        "        counts.update(tokens)  # increments counts for all items in tokens\n",
        "    print('counts=', counts)   # for debugging\n",
        "    # Normalize so probabilities sum to 1.\n",
        "    total_tokens = sum(counts.values())\n",
        "    return {token: value / total_tokens for\n",
        "                                        token, value in counts.items()}\n",
        "\n",
        "unigram_probs = estimate_unigram_probs(docs)\n",
        "# print in descending order.\n",
        "sorted(unigram_probs.items(), key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPo5c6LtVp5u"
      },
      "source": [
        "## Probability of a Whole Sentence\n",
        "\n",
        "\n",
        "Once we've estimated unigram probabilities, we can now compute the probability of a new sentence with $m$ words by the product of its unigram probabilities.\n",
        "\n",
        "$p(w_1 \\ldots w_m) \\approx p(w_1) * p(w_2) * \\ldots p(w_m)$\n",
        "\n",
        "(True because of our earlier independence assumption.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_zxpVPgVn61"
      },
      "outputs": [],
      "source": [
        "def sentence_probability(unigrams, sentence):\n",
        "    proba = 1\n",
        "    for word in sentence:\n",
        "        proba *= unigrams[word]\n",
        "    return proba\n",
        "\n",
        "import math\n",
        "def sentence_log_probability(unigrams, sentence):\n",
        "    proba = 0\n",
        "    for word in sentence:\n",
        "        proba += math.log10(unigrams[word])\n",
        "    return proba\n",
        "\n",
        "sentence_probability(unigram_probs, ['I', 'Sam', 'am'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbztndt_Vzp5"
      },
      "outputs": [],
      "source": [
        "sentence_probability(unigram_probs, ['Sam', 'I', 'I'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4ooNklGV0UJ"
      },
      "outputs": [],
      "source": [
        "# What happens for very very long sentences?\n",
        "\n",
        "sentence_probability(unigram_probs, ['Sam'] * 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qjNLUM7V9kY"
      },
      "source": [
        "## N-gram Language Models\n",
        "\n",
        "$p(w_1 \\ldots w_m) \\approx p(w_1) * p(w_2) * \\ldots p(w_m)$\n",
        "\n",
        "\n",
        "Why is this a bad assumption?\n",
        "\n",
        "<br><br><br>\n",
        "Ignores all structure of language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIy7DsSWV57N"
      },
      "outputs": [],
      "source": [
        "## e.g., cannot tell that the first sentence is more likely than the second.\n",
        "sentence_log_probability(unigram_probs, ['Sam', 'I', 'am']) == \\\n",
        "sentence_log_probability(unigram_probs, ['Sam', 'Sam', 'I'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c74sP9F1WLnn"
      },
      "source": [
        "Really, we'd like a rich syntactic/semantic representation with probabilities assigned.  \n",
        "\n",
        "\n",
        "We'll get to more complex representations in the future, but for now, we can do better by at least modeling **n-grams**, instead of just unigrams.\n",
        "\n",
        "## The Markov Assumption\n",
        "\n",
        "What is the probability of the $i$th word in a sentence, given all the previous words?\n",
        "$$ p(w_i  | w_1 w_2 \\ldots w_{i-1})?$$\n",
        "\n",
        "We will make an assumption that the $i$th word depends only on the previous $n$ words (**Markov** assumption):\n",
        "\n",
        "$$ p(w_i | w_1 w_2 \\ldots w_{i-1}) \\approx p(w_i | w_{i-1} w_{i-2} \\ldots w_{i-n}) $$\n",
        "\n",
        "e.g., 4-grams:\n",
        "\n",
        "$p($ \"flu\" $ | $ \"He is sick with the\" $ ) $ $\\approx p($\"flu\"$ | $ \"sick with the\" $)$\n",
        "\n",
        "$$ p(w_i | w_{i-1} w_{i-2} \\ldots w_{i-n}) = \\frac{C(w_{i-n}, \\ldots, w_{i-2} w_{i-1} w_i)}{C(w_{i-n} \\ldots w_{i-2} w_{i-1})} $$\n",
        "\n",
        "<br>\n",
        "Can estimate from data. E.g., search engines:\n",
        "\n",
        "= Num hits for [\"sick with the flu\"](https://www.google.com/search?q=%22i+am+sick+with+the+flu%22) / Num hits for [\"sick with the\"](https://www.google.com/search?q=%22i+am+sick+with+the%22)  \n",
        "$ \\approx 18,600 / 82,500 \\approx 0.23$\n",
        "\n",
        "<br><br>\n",
        "See also [Google's NGram corpus](https://books.google.com/ngrams/graph?content=sick+with+the+flu+%2F+sick+with+the&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=&direct_url=t1%3B%2C%28sick%20with%20the%20flu%20/%20sick%20with%20the%29%3B%2Cc0)\n",
        "- Ngram statistics extracted from millions of books.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrZEeq8WXH0"
      },
      "source": [
        "## N-Gram Model Implmentation\n",
        "\n",
        "Let's start with a simpler example:\n",
        "\n",
        "\n",
        "```\n",
        "<s> I am Sam </s>  \n",
        "<s> Sam I am </s>  \n",
        "<s> I do not like green eggs and ham <s>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMf4wkFHWEYQ"
      },
      "outputs": [],
      "source": [
        "def iter_ngrams(doc, n):\n",
        "    \"\"\"Return a generator over ngrams of a document.\n",
        "    Params:\n",
        "      doc...list of tokens\n",
        "      n.....size of ngrams\"\"\"\n",
        "    return (doc[i : i+n] for i in range(len(doc)-n+1))\n",
        "\n",
        "[ngram for ngram in\n",
        "           iter_ngrams(['<s>', 'hi', 'there', 'how', 'are', 'you', '</s>'], 3)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqNoQFIzWcpz"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "def estimate_ngram_probs(docs, n, verbose=True):\n",
        "    \"\"\"Compute p(w_i|w_i-1, w_i-2, ..., w_i-n), the probability of each ngram in this data.\n",
        "    = count(w_i-1, w_i-2, ..., w_i-n, w_i) / count(w_i-1, w_i-2, ..., w_i-n)\n",
        "\n",
        "    Params:\n",
        "      docs...list of strings\n",
        "      n......ngram size\"\"\"\n",
        "    # Count\n",
        "    counts = defaultdict(lambda: Counter())\n",
        "    for doc in docs:\n",
        "        for ngram in iter_ngrams(doc.split(), n):\n",
        "            counts[tuple(ngram[:-1])].update([ngram[-1]])\n",
        "\n",
        "    if verbose:\n",
        "        print('counts=\\n', '\\n'.join(str(i) for i in counts.items()))   # for debugging\n",
        "\n",
        "    # Normalize probabilities to sum to 1.0\n",
        "    for ngram, word_counts in counts.items():\n",
        "        total = sum(word_counts.values())\n",
        "        counts[ngram] = {word: count / total\n",
        "                         for word, count in word_counts.items()}\n",
        "    return counts\n",
        "\n",
        "sam_ngrams = estimate_ngram_probs(docs, 3)\n",
        "sam_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVac5ZE8WlSM"
      },
      "outputs": [],
      "source": [
        "estimate_ngram_probs(docs, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqIvas3NWpLV"
      },
      "outputs": [],
      "source": [
        "# Pr(Sam | \"I am\")\n",
        "sam_ngrams[('I', 'am')]['Sam']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8AnbOMqWt-o"
      },
      "source": [
        "## Probability of a Sentence with n-grams\n",
        "\n",
        "As before, we can compute the probability of a sentence by multiplying probabilities of all ngrams in the sentence.\n",
        "\n",
        "$$p(w_1 \\ldots w_m) = p(w_n|w_{n-1} \\ldots w_1) * p(w_{n+1}|w_{n} \\ldots w_2) * \\ldots * p(w_m|w_{m-1}\\ldots w_{m-n})$$\n",
        "\n",
        "e.g. for $n=2$\n",
        "\n",
        "$p($ &langle;s&rangle; Sam I am &langle;/s&rangle;$) = p($ Sam $\\mid $ &langle;s&rangle; $) * p($I $ \\mid $ Sam $) * p($ am $ \\mid $ I $) *  p($ &langle;/s&rangle; $ \\mid $ am $ ) $\n",
        "\n",
        "And equivalently in log space:\n",
        "\n",
        "$$\\log p(w_1 \\ldots w_m) = \\log p(w_n|w_{n-1} \\ldots w_1) + \\log p(w_{n+1}|w_{n} \\ldots w_2) + \\ldots + \\log p(w_m|w_{m-1} \\ldots w_{m-n})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftInoL_bWren"
      },
      "outputs": [],
      "source": [
        "def sentence_log_probability_ngrams(ngrams, sentence, n):\n",
        "    proba = 0\n",
        "    # loop through all ngrams\n",
        "    for ngram in iter_ngrams(sentence, n):\n",
        "        # e.g., ngram = ['Sam', 'I', 'am']\n",
        "        #      prefix = ['Sam', 'I']\n",
        "        #        word = 'am'f\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        # P(wi | ngram)\n",
        "        this_prob = math.log10(ngrams[tuple(prefix)][word])\n",
        "        print('Pr(%s|%s)=%g' % (word, prefix, this_prob))\n",
        "        proba += this_prob\n",
        "    return proba\n",
        "\n",
        "sentence_log_probability_ngrams(sam_ngrams, ['<s>', 'Sam', 'I', 'am', '</s>'], 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YMtBqnVW3bM"
      },
      "source": [
        "# Let's Make a Donald Trump Bot...\n",
        "\n",
        "Let's download a sample of Donald Trump's tweets and fit an n-gram language model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU7A8N6ZW0NQ"
      },
      "outputs": [],
      "source": [
        "# download pre-processed data here.\n",
        "!wget \"https://github.com/tulane-cmps6730/main/raw/refs/heads/main/lec/language_models/tweets.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o_mofdUW8e6"
      },
      "outputs": [],
      "source": [
        "!head tweets.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHZtAd99W922"
      },
      "outputs": [],
      "source": [
        "trump_tweets = open('tweets.txt').readlines()\n",
        "print('read %d tweets from Donald J. Trump (@realDonaldTrump)' % len(trump_tweets))\n",
        "print(trump_tweets[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukYlc5EgW_YE"
      },
      "outputs": [],
      "source": [
        "## Note the second parameter is the total n-gram size, so word + window... if we\n",
        "## want two words of size then we need to do 3\n",
        "\n",
        "trump_ngrams = estimate_ngram_probs(trump_tweets, 3, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP8broavXCKc"
      },
      "outputs": [],
      "source": [
        "# What does Trump think about the media?\n",
        "sorted(trump_ngrams[('media', 'is')].items(), key=lambda x: -x[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN2n6mORXDze"
      },
      "outputs": [],
      "source": [
        "# What does Trump think about the Hillary Clinton?\n",
        "sorted(trump_ngrams[('clinton', 'is')].items(), key=lambda x: -x[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7J8fUdfX1lV"
      },
      "outputs": [],
      "source": [
        "trump_ngrams[('clinton', 'is')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYBkUGmEXO9E"
      },
      "source": [
        "# Sampling from ngram Language Model\n",
        "\n",
        "\n",
        "Given probabilities $p(w_i | w_{i-1} w_{i-2} \\ldots w_{i-n})$ $\\forall i$, how can we sample sentences?\n",
        "\n",
        "\n",
        "<br><br><br><br>\n",
        "1. Sample an ngram $(w_1\\ldots w_n)$ where $w_1 = $ &langle;s&rangle;\n",
        "2. Sample an additional word from $p(w_i | w_{i-1} w_{i-2} \\ldots w_{i-n})$\n",
        "3. Loop until sample &langle;/s&rangle;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dnKXILLXHmo"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# Sampling uniformly from a list.\n",
        "random.sample(['a', 'b', 'c'], k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XkF9A_oXVRH"
      },
      "outputs": [],
      "source": [
        "# Sampling from a multinomial distribution\n",
        "np.random.choice(['a', 'b', 'c'], size=10, p=[.7, .2, .1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMKIikoZXWeB"
      },
      "outputs": [],
      "source": [
        "# A simple TrumpBot\n",
        "import random\n",
        "\n",
        "def generate_sentences(ngrams, k):\n",
        "    \"\"\"Sample k sentences from given ngram model.\n",
        "    Params:\n",
        "      ngrams....ngram language model; a dict from ngram tuple to a dict\n",
        "      k.........number of sentences to sample\n",
        "    \"\"\"\n",
        "    # List of all ngrams that start with <s>\n",
        "    start_ngrams = [ngram for ngram in ngrams if ngram[0] == '<s>']\n",
        "    for i in range(k):  # sample k sentences.\n",
        "        # sample uniformly from all start ngrams.\n",
        "        ngram = random.sample(start_ngrams, 1)[0]\n",
        "        sentence = []\n",
        "        sentence.extend(ngram)\n",
        "        while sentence[-1] != '</s>' and len(sentence) < 15:  # while not at end of sentence.\n",
        "            # sample the next word\n",
        "            sampled_word = np.random.choice(list(ngrams[ngram].keys()),      # words\n",
        "                                            p=list(ngrams[ngram].values()),  # probabilities\n",
        "                                            size=1)[0]\n",
        "            sentence.append(sampled_word)\n",
        "            # update most recent ngram\n",
        "            ngram = tuple(list(ngram[1:]) + [sampled_word])\n",
        "        print(' '.join(sentence))\n",
        "\n",
        "generate_sentences(trump_ngrams, 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N99JV6-fXYKR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
